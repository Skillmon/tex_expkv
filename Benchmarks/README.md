# Benchmarks

The files in this folder can be used to compare the performance of different
key=value implementation.

I don't intend to fully document the files in these folders. Here's what you can
do:

- Run `run_benchmark.sh` to run a (long) benchmark of the available key=value
  implementations.

- Run `read_results.py` on a results file generated by `run_benchmark.sh`
  (default name is `benchmark_results.txt`) to calculate the polynomial
  coefficients of the least square fits to the data points. You might also write
  your own evaluation script for which you could use the `read_results` function
  provided in that file.

- Run `benchmarks.tex` to directly run the benchmark in LaTeX. No file storing
  the results will be created. The LaTeX script can also be used to test for the
  unbracing bug common to many key=value packages (uncomment `\testbugtrue`) or
  to test whether they can cope with category code changes of commas and equals
  (uncomment `\testcattrue`, this will throw errors from the packages that fail
  this test). You can also deactivate the benchmarking by uncommenting
  `\benchmarkfalse`, but this way the `run_benchmark.sh` script will no longer
  work correctly.
